{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d31f1b76-1b88-46dc-8c5b-f853eb648a17",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from newsapi import NewsApiClient\n",
    "import joblib\n",
    "import json\n",
    "import pickle\n",
    "import h5py\n",
    "from sklearn.metrics import f1_score\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5726a20a-6913-45cf-83f5-f35cd19beac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"csv_file/data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050f9d92-5aae-46ca-856c-53021d408297",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_words = []\n",
    "count_words_alnum = []\n",
    "for i in df[\"headlines\"]:\n",
    "    word = i.split()\n",
    "    word = ' '.join(letter for letter in word if letter.isalnum())\n",
    "    count_words.append(word)\n",
    "\n",
    "for i in count_words:\n",
    "    i = i.split(\" \")\n",
    "    count_words_alnum.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf00e66-ce79-4f3f-a1c6-20d5a3392ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_extend(matrix):\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list.extend(row)\n",
    "    return flat_list\n",
    "\n",
    "flatten_list = flatten_extend(count_words_alnum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003ddde-a00e-43e3-b2a5-fb70e065efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_list_unique = set(flatten_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941ab0a4-c2c0-4977-8564-44326e89c58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(flatten_list_unique))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73482012-46ae-466e-94f7-0698297d8f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, i in enumerate(df[\"headlines\"]):\n",
    "    row = df[\"headlines\"].iloc[index]\n",
    "    row = row.split(\" \")\n",
    "    row = ' '.join(letter for letter in row if letter.isalnum() or letter == \"COVID-19\")\n",
    "    if \"COVID-19\" in row:\n",
    "        row = row.replace(\"COVID-19\", \"COVID19\")\n",
    "    replace = df[\"headlines\"].iloc[index]\n",
    "    df = df.replace(to_replace=replace, value=row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc3d71-7288-463a-af90-935b24e326f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(15):\n",
    "    print(df[\"headlines\"].iloc[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a837e24-b565-457d-a3f9-afddd8a9eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_set = set(count_words)\n",
    "unique_list = (list(list_set))\n",
    "print(len(unique_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83fd127-9781-4f07-8efd-e89d7c1cb884",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data (you should replace this with your dataset)\n",
    "texts = df[\"headlines\"]\n",
    "labels = df[\"outcome\"]\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "max_words = 12137  # Number of unique words to keep\n",
    "maxlen = 75  # Maximum length of sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d608472-28eb-4e11-9a32-8b3c63c67524",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ed3d8a-67d8-439e-b956-77175bbf9445",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcb95fbe-b593-455d-9132-84f069d2229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e657b9b9-7818-406a-a959-ef5403d7c52e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c32b34-ab78-47f9-a172-33cbcbcfc40d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c5fab2-9c40-48ae-9dfa-63779cceb325",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5d9f4e-4f8d-49f8-8b5e-32a7f4613b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3298e5a-643b-436d-8eec-97710998cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_word_index = {word: index for word, index in word_index.items() if word.lower() not in stop_words}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0869f97c-1272-4298-89a9-40d890aea668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bf7011-c3f3-4ae5-95af-5ccf80cb6d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index = filtered_word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a729b14-e998-4dbb-b232-f81cdd53838d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a025fd6-3255-4dda-b6a2-a7c97e868bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_lengths = [len(seq) for seq in sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c041f2-3b75-43b3-b7b1-b61e9239556d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistics.mean(sequence_lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0c1720-c0d7-4cfa-ba4f-2db4298eda0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X = pad_sequences(sequences, maxlen=maxlen)\n",
    "y = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5eaceb-fd11-4b9c-b1f4-1b75475cd0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the RNN model\n",
    "embedding_dim = 50  # Dimensionality of the embedding space\n",
    "hidden_units = 50  # Number of LSTM units\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, embedding_dim, input_length=maxlen))\n",
    "model.add(LSTM(hidden_units))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)\n",
    "\n",
    "# Train the model\n",
    "epochs = 5\n",
    "batch_size = 32\n",
    "\n",
    "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57adde43-8cd9-4efb-9a88-597993478595",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425a210-a97e-4cc5-b483-a1d3b46b6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(int)\n",
    "\n",
    "# Compute confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "\n",
    "# Display confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=['Fake', 'Real'], yticklabels=['Fake', 'Real'])\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Display classification report\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dead3660-0f8e-4513-ba0a-eb2661f2282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "newsapi = NewsApiClient(api_key='2ec72918ca08491b885785649a92cfb7')  # Replace with your actual News API key\n",
    "\n",
    "# /v2/top-headlines\n",
    "covid_headlines = newsapi.get_top_headlines(q='covid',\n",
    "                                            language='en',\n",
    "                                            country='us')\n",
    "\n",
    "# /v2/everything\n",
    "covid_articles = newsapi.get_everything(q='covid',\n",
    "                                        language='en',\n",
    "                                        sort_by='relevancy',\n",
    "                                        page=1)\n",
    "\n",
    "\n",
    "title = []\n",
    "clean_title = []\n",
    "for article in covid_articles['articles']:\n",
    "    print(article['title'])\n",
    "    title.append(article['title'])\n",
    "\n",
    "for index, i in enumerate(title):\n",
    "    row = title[index]\n",
    "    row = row.split(\" \")\n",
    "    row = ' '.join(letter for letter in row if letter.isalnum() or letter == \"COVID-19\")\n",
    "    if \"COVID-19\" in row:\n",
    "        row = row.replace(\"COVID-19\", \"COVID19\")\n",
    "    row = row.lower()\n",
    "    clean_title.append(row)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438e975a-ad4e-4466-8aa7-00ba02d00bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de36384-9b76-432f-80d8-be71584b699f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# News API setup\n",
    "newsapi = NewsApiClient(api_key='2ec72918ca08491b885785649a92cfb7')  # Replace with your actual News API key\n",
    "\n",
    "# Query the News API\n",
    "news_data = newsapi.get_everything(q='covid',\n",
    "                                        language='en',\n",
    "                                        sort_by='relevancy',\n",
    "                                        page=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96dd22-7be0-4f92-916b-8c866d688390",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    sequences = tokenizer.texts_to_sequences([text])\n",
    "    padded_sequence = pad_sequences(sequences, maxlen=maxlen)  # Ensure maxlen matches the length used during training\n",
    "    print(padded_sequence)\n",
    "    return padded_sequence\n",
    "\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "# Process and predict for each news article\n",
    "for index, article in enumerate(clean_title):\n",
    "    title = article\n",
    "    if \"covid\" in title.lower():\n",
    "        true_labels.append(1)\n",
    "        print(title)\n",
    "    else:\n",
    "        true_labels.append(0)\n",
    "   \n",
    "        \n",
    "    # Preprocess the text\n",
    "    processed_title = preprocess_text(title)\n",
    "    \n",
    "    # Make predictions\n",
    "    prediction = model.predict(processed_title)\n",
    "    if prediction < 0.5:\n",
    "        predicted_labels.append(0)\n",
    "    else:\n",
    "        predicted_labels.append(1)\n",
    "    \n",
    "   \n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Prediction: {'Fake' if prediction < 0.5 else 'Real'}\")\n",
    "    print(\"=\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c02c737-f60c-4a4a-9a64-7bfe12a82f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(true_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29321c48-161c-4857-ba50-bda539ab8c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
